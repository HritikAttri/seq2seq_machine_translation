{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled41.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9iKe1X-zMZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFRn9J1NzpKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_over_time(x):\n",
        "    assert(K.ndim(x) > 2)\n",
        "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
        "    s = K.sum(e, axis=1, keepdims=True)\n",
        "    return e / s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL5Q2Leozwe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_SAMPLES = 10000\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih0JRSfnzwhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "471914f2-1ba5-409b-8695-4c02208da578"
      },
      "source": [
        "# Load the data\n",
        "input_texts = []           # Sentence in original language\n",
        "target_texts = []          # Sentence in target language\n",
        "target_texts_inputs = []        # Sentence in target language with offset\n",
        "\n",
        "count = 0\n",
        "\n",
        "with open(\"drive//My Drive//data_science//chatbot_data//machine_translation//spa.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        if line:\n",
        "            count += 1\n",
        "            if count > MAX_NUM_SAMPLES:\n",
        "                break\n",
        "                \n",
        "            input_sentence, translation, _ = line.split(\"\\t\")\n",
        "            \n",
        "            target_sentence = translation + \" <eos>\"\n",
        "            target_sentence_input = \"<sos> \" + translation\n",
        "            \n",
        "            input_texts.append(input_sentence)\n",
        "            target_texts.append(target_sentence)\n",
        "            target_texts_inputs.append(target_sentence_input)\n",
        "            \n",
        "print(\"Number of samples in data: \", len(input_texts))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in data:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAmEA1KDzwmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5dd3f692-1a56-4f90-e218-43d566a61414"
      },
      "source": [
        "# Tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# Word to index, and index to word mapping\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "idx2word_inputs = {v: k for k, v in word2idx_inputs.items()}\n",
        "\n",
        "max_input_len = max([len(s) for s in input_sequences])\n",
        "\n",
        "# Tokenize the outputs(keep sos and eos tokens, so dont filter)\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=\"\")\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
        "\n",
        "# Word to index, and index to word mapping\n",
        "word2idx_targets = tokenizer_outputs.word_index\n",
        "idx2word_targets = {v: k for k, v in word2idx_targets.items()}\n",
        "\n",
        "num_words_target = len(word2idx_targets) + 1\n",
        "\n",
        "max_target_len = max([len(s) for s in target_sequences])\n",
        "\n",
        "# Pad the sequences(we don't add zero padding at the end for encoder, as it helps decoder.)\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_input_len)\n",
        "print(\"Encoder input data shape: \", encoder_inputs.shape)\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_target_len, padding=\"post\")\n",
        "print(\"Decoder input data shape: \", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_target_len, padding=\"post\")\n",
        "print(\"Decoder target data shape: \", decoder_targets.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder input data shape:  (10000, 5)\n",
            "Decoder input data shape:  (10000, 9)\n",
            "Decoder target data shape:  (10000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgljm5pVzwrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fdaf8064-a636-45d4-c428-a92de5debd1e"
      },
      "source": [
        "# Load word vectors\n",
        "print(\"Loading word vectors...\")\n",
        "word2vec = {}\n",
        "with open(\"drive//My Drive//data_science//word_embeddings//glove.6B.{}d.txt\".format(EMBEDDING_DIM), encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.asarray(values[1:], dtype=\"float32\")\n",
        "        word2vec[word] = vec\n",
        "print(\"Loaded {} word vectors.\".format(len(word2vec)))\n",
        "\n",
        "# Prepare embeddings\n",
        "print(\"Filling pre-trained embeddings...\")\n",
        "num_words_input = min(MAX_VOCAB_SIZE, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words_input, EMBEDDING_DIM))                # Shape -> (vocab_size, embedding_dim)\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i < MAX_VOCAB_SIZE:\n",
        "        embedding_vector = word2vec.get(word)       # Returns None if not found\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "print(\"Filled {} pre-trained embeddings.\".format(embedding_matrix.shape[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Loaded 400000 word vectors.\n",
            "Filling pre-trained embeddings...\n",
            "Filled 2351 pre-trained embeddings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRGM3WaSzww-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encode targets as we can't use sparse cross entropy when each sample has multiple targets\n",
        "decoder_one_hot_targets = np.zeros((MAX_NUM_SAMPLES, max_target_len, num_words_target))\n",
        "for i, decoder_target in enumerate(decoder_targets):\n",
        "    for t, word in enumerate(decoder_target):\n",
        "        if int(word) > 0:\n",
        "            decoder_one_hot_targets[i, t, int(word)] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQilcgVJzw0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the encoder model\n",
        "\n",
        "# Inputs\n",
        "encoder_inputs_model = Input(shape=(max_input_len, ))\n",
        "\n",
        "# Model\n",
        "encoder_embedding_layer = Embedding(num_words_input, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_input_len)\n",
        "encoder_x = encoder_embedding_layer(encoder_inputs_model)\n",
        "encoder_lstm = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, dropout=0.5))\n",
        "encoder_outputs = encoder_lstm(encoder_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg4sAJVhzwvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the decoder model\n",
        "\n",
        "# Inputs\n",
        "decoder_inputs_model = Input(shape=(max_target_len, ))\n",
        "\n",
        "# Model(Part - I)\n",
        "decoder_embedding_layer = Embedding(num_words_target, EMBEDDING_DIM)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs_model)\n",
        "\n",
        "# Attention\n",
        "attn_repeat_layer = RepeatVector(max_input_len)\n",
        "attn_concat_layer = Concatenate(axis=-1)\n",
        "attn_dense1 = Dense(10, activation=\"tanh\")\n",
        "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
        "attn_dot = Dot(axes=1)\n",
        "\n",
        "def one_step_attention(h, st_1):\n",
        "    # Copy state s(t-1) max_input_len times\n",
        "    st_1 = attn_repeat_layer(st_1)\n",
        "    # Concatenate all h(t)'s with s(t-1)\n",
        "    x = attn_concat_layer([h, st_1])\n",
        "    # Neural network layer 1\n",
        "    x = attn_dense1(x)\n",
        "    # Get alpha values\n",
        "    alphas = attn_dense2(x)\n",
        "    # Get context\n",
        "    context = attn_dot([alphas, h])\n",
        "    \n",
        "    return context\n",
        "\n",
        "# Model(Part - II)\n",
        "decoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
        "decoder_dense = Dense(num_words_target, activation=\"softmax\")\n",
        "\n",
        "initial_s = Input(shape=(LATENT_DIM, ), name=\"s0\")\n",
        "initial_c = Input(shape=(LATENT_DIM, ), name=\"c0\")\n",
        "context_last_word_concat_layer = Concatenate(axis=2)\n",
        "\n",
        "s = initial_s\n",
        "c = initial_c\n",
        "\n",
        "outputs = []\n",
        "for t in range(max_target_len):\n",
        "    # Get the context using attention\n",
        "    context = one_step_attention(encoder_outputs, s)\n",
        "    # We need to select one word of input only, not the entire sequence of input\n",
        "    selector = Lambda(lambda x: x[:, t:t+1])\n",
        "    x_t = selector(decoder_embeddings)\n",
        "    # Concatenate\n",
        "    decoder_lstm_input = context_last_word_concat_layer([context, x_t])\n",
        "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
        "    decoder_outputs = decoder_dense(o)\n",
        "    \n",
        "    outputs.append(decoder_outputs)\n",
        "    \n",
        "def stack_and_transpose(x):\n",
        "    \"\"\"Shape of x: [max_target_len, batch_size, num_words_target]. We need the batch_size first.\"\"\"\n",
        "    x = K.stack(x)\n",
        "    x = K.permute_dimensions(x, pattern=(1, 0, 2))\n",
        "    return x\n",
        "\n",
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)\n",
        "\n",
        "model = Model(inputs=[encoder_inputs_model, decoder_inputs_model, initial_s, initial_c], outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1x7ofJtzwqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeDhil1ozwlL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b056a081-b88e-4fd1-e544-6ca0ac162f9b"
      },
      "source": [
        "z = np.zeros((len(encoder_inputs), LATENT_DIM))\n",
        "result = model.fit([encoder_inputs, decoder_inputs, z, z], decoder_one_hot_targets, batch_size=BATCH_SIZE, \n",
        "                   epochs=EPOCHS, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 44s 706ms/step - loss: 2.6864 - accuracy: 0.1093 - val_loss: 2.9426 - val_accuracy: 0.1111\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 41s 658ms/step - loss: 2.3299 - accuracy: 0.1122 - val_loss: 2.9091 - val_accuracy: 0.1231\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 2.2565 - accuracy: 0.1187 - val_loss: 2.8910 - val_accuracy: 0.1279\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 2.2060 - accuracy: 0.1217 - val_loss: 2.8679 - val_accuracy: 0.1322\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 2.1695 - accuracy: 0.1236 - val_loss: 2.8536 - val_accuracy: 0.1325\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 41s 654ms/step - loss: 2.1370 - accuracy: 0.1242 - val_loss: 2.8190 - val_accuracy: 0.1345\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 42s 669ms/step - loss: 2.0963 - accuracy: 0.1259 - val_loss: 2.7756 - val_accuracy: 0.1378\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 42s 671ms/step - loss: 2.0556 - accuracy: 0.1297 - val_loss: 2.7409 - val_accuracy: 0.1402\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 41s 655ms/step - loss: 2.0145 - accuracy: 0.1314 - val_loss: 2.7230 - val_accuracy: 0.1414\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 41s 658ms/step - loss: 1.9780 - accuracy: 0.1328 - val_loss: 2.7079 - val_accuracy: 0.1409\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 41s 652ms/step - loss: 1.9423 - accuracy: 0.1333 - val_loss: 2.6995 - val_accuracy: 0.1419\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 1.9101 - accuracy: 0.1344 - val_loss: 2.6753 - val_accuracy: 0.1466\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 1.8732 - accuracy: 0.1369 - val_loss: 2.6589 - val_accuracy: 0.1482\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 41s 653ms/step - loss: 1.8393 - accuracy: 0.1392 - val_loss: 2.6509 - val_accuracy: 0.1478\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 41s 653ms/step - loss: 1.8076 - accuracy: 0.1416 - val_loss: 2.6266 - val_accuracy: 0.1496\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 42s 659ms/step - loss: 1.7756 - accuracy: 0.1437 - val_loss: 2.6089 - val_accuracy: 0.1532\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 42s 659ms/step - loss: 1.7452 - accuracy: 0.1455 - val_loss: 2.5979 - val_accuracy: 0.1520\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 41s 658ms/step - loss: 1.7159 - accuracy: 0.1487 - val_loss: 2.5783 - val_accuracy: 0.1586\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 41s 655ms/step - loss: 1.6816 - accuracy: 0.1525 - val_loss: 2.5651 - val_accuracy: 0.1577\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 42s 659ms/step - loss: 1.6522 - accuracy: 0.1549 - val_loss: 2.5430 - val_accuracy: 0.1631\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 41s 657ms/step - loss: 1.6212 - accuracy: 0.1575 - val_loss: 2.5322 - val_accuracy: 0.1691\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 41s 655ms/step - loss: 1.5901 - accuracy: 0.1600 - val_loss: 2.5140 - val_accuracy: 0.1656\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 42s 660ms/step - loss: 1.5582 - accuracy: 0.1622 - val_loss: 2.4900 - val_accuracy: 0.1721\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 1.5270 - accuracy: 0.1657 - val_loss: 2.4728 - val_accuracy: 0.1749\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 41s 652ms/step - loss: 1.4972 - accuracy: 0.1673 - val_loss: 2.4563 - val_accuracy: 0.1776\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 41s 645ms/step - loss: 1.4643 - accuracy: 0.1697 - val_loss: 2.4413 - val_accuracy: 0.1803\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 1.4332 - accuracy: 0.1717 - val_loss: 2.4330 - val_accuracy: 0.1801\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 41s 646ms/step - loss: 1.4019 - accuracy: 0.1743 - val_loss: 2.4182 - val_accuracy: 0.1814\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 1.3693 - accuracy: 0.1760 - val_loss: 2.4049 - val_accuracy: 0.1852\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 1.3384 - accuracy: 0.1786 - val_loss: 2.3928 - val_accuracy: 0.1863\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 41s 653ms/step - loss: 1.3092 - accuracy: 0.1806 - val_loss: 2.3826 - val_accuracy: 0.1885\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 1.2803 - accuracy: 0.1835 - val_loss: 2.3662 - val_accuracy: 0.1924\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 1.2482 - accuracy: 0.1859 - val_loss: 2.3576 - val_accuracy: 0.1941\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 1.2176 - accuracy: 0.1875 - val_loss: 2.3502 - val_accuracy: 0.1939\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 1.1900 - accuracy: 0.1895 - val_loss: 2.3348 - val_accuracy: 0.1969\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 41s 646ms/step - loss: 1.1621 - accuracy: 0.1927 - val_loss: 2.3399 - val_accuracy: 0.1961\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 1.1328 - accuracy: 0.1941 - val_loss: 2.3193 - val_accuracy: 0.1993\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 1.1046 - accuracy: 0.1963 - val_loss: 2.3075 - val_accuracy: 0.2003\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 1.0767 - accuracy: 0.1989 - val_loss: 2.2994 - val_accuracy: 0.2017\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 41s 646ms/step - loss: 1.0502 - accuracy: 0.2007 - val_loss: 2.2906 - val_accuracy: 0.2019\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 1.0227 - accuracy: 0.2036 - val_loss: 2.2868 - val_accuracy: 0.2028\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 0.9969 - accuracy: 0.2052 - val_loss: 2.2752 - val_accuracy: 0.2031\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 0.9712 - accuracy: 0.2085 - val_loss: 2.2683 - val_accuracy: 0.2051\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 0.9466 - accuracy: 0.2110 - val_loss: 2.2651 - val_accuracy: 0.2061\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 0.9229 - accuracy: 0.2128 - val_loss: 2.2569 - val_accuracy: 0.2074\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 41s 652ms/step - loss: 0.9014 - accuracy: 0.2153 - val_loss: 2.2552 - val_accuracy: 0.2054\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.8792 - accuracy: 0.2172 - val_loss: 2.2548 - val_accuracy: 0.2069\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 0.8552 - accuracy: 0.2202 - val_loss: 2.2443 - val_accuracy: 0.2071\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 41s 645ms/step - loss: 0.8347 - accuracy: 0.2227 - val_loss: 2.2415 - val_accuracy: 0.2074\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.8115 - accuracy: 0.2251 - val_loss: 2.2341 - val_accuracy: 0.2077\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 0.7946 - accuracy: 0.2278 - val_loss: 2.2413 - val_accuracy: 0.2083\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 41s 654ms/step - loss: 0.7742 - accuracy: 0.2314 - val_loss: 2.2324 - val_accuracy: 0.2086\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.7520 - accuracy: 0.2330 - val_loss: 2.2283 - val_accuracy: 0.2092\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.7367 - accuracy: 0.2361 - val_loss: 2.2274 - val_accuracy: 0.2105\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 0.7154 - accuracy: 0.2396 - val_loss: 2.2329 - val_accuracy: 0.2108\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.7019 - accuracy: 0.2407 - val_loss: 2.2226 - val_accuracy: 0.2091\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.6825 - accuracy: 0.2446 - val_loss: 2.2209 - val_accuracy: 0.2114\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 0.6657 - accuracy: 0.2472 - val_loss: 2.2179 - val_accuracy: 0.2092\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 41s 643ms/step - loss: 0.6518 - accuracy: 0.2490 - val_loss: 2.2200 - val_accuracy: 0.2102\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 0.6355 - accuracy: 0.2517 - val_loss: 2.2201 - val_accuracy: 0.2097\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.6220 - accuracy: 0.2535 - val_loss: 2.2179 - val_accuracy: 0.2104\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.6063 - accuracy: 0.2574 - val_loss: 2.2165 - val_accuracy: 0.2103\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 41s 643ms/step - loss: 0.5932 - accuracy: 0.2595 - val_loss: 2.2138 - val_accuracy: 0.2128\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.5820 - accuracy: 0.2622 - val_loss: 2.2145 - val_accuracy: 0.2106\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 41s 645ms/step - loss: 0.5666 - accuracy: 0.2647 - val_loss: 2.2195 - val_accuracy: 0.2098\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.5515 - accuracy: 0.2681 - val_loss: 2.2132 - val_accuracy: 0.2123\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 41s 648ms/step - loss: 0.5409 - accuracy: 0.2688 - val_loss: 2.2140 - val_accuracy: 0.2108\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 0.5283 - accuracy: 0.2714 - val_loss: 2.2189 - val_accuracy: 0.2080\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 41s 644ms/step - loss: 0.5203 - accuracy: 0.2727 - val_loss: 2.2201 - val_accuracy: 0.2127\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 0.5066 - accuracy: 0.2758 - val_loss: 2.2249 - val_accuracy: 0.2124\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.4980 - accuracy: 0.2779 - val_loss: 2.2169 - val_accuracy: 0.2111\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4856 - accuracy: 0.2802 - val_loss: 2.2137 - val_accuracy: 0.2123\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.4747 - accuracy: 0.2817 - val_loss: 2.2227 - val_accuracy: 0.2116\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4659 - accuracy: 0.2829 - val_loss: 2.2214 - val_accuracy: 0.2138\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4562 - accuracy: 0.2853 - val_loss: 2.2252 - val_accuracy: 0.2112\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 41s 655ms/step - loss: 0.4487 - accuracy: 0.2863 - val_loss: 2.2194 - val_accuracy: 0.2125\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4398 - accuracy: 0.2883 - val_loss: 2.2237 - val_accuracy: 0.2126\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 41s 654ms/step - loss: 0.4293 - accuracy: 0.2898 - val_loss: 2.2270 - val_accuracy: 0.2121\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4242 - accuracy: 0.2912 - val_loss: 2.2325 - val_accuracy: 0.2127\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.4145 - accuracy: 0.2931 - val_loss: 2.2252 - val_accuracy: 0.2127\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 41s 654ms/step - loss: 0.4059 - accuracy: 0.2953 - val_loss: 2.2307 - val_accuracy: 0.2133\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.4001 - accuracy: 0.2961 - val_loss: 2.2299 - val_accuracy: 0.2114\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 41s 652ms/step - loss: 0.3933 - accuracy: 0.2974 - val_loss: 2.2339 - val_accuracy: 0.2137\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 41s 649ms/step - loss: 0.3839 - accuracy: 0.2989 - val_loss: 2.2387 - val_accuracy: 0.2129\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 41s 653ms/step - loss: 0.3807 - accuracy: 0.2997 - val_loss: 2.2406 - val_accuracy: 0.2147\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.3711 - accuracy: 0.3012 - val_loss: 2.2412 - val_accuracy: 0.2134\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 41s 647ms/step - loss: 0.3652 - accuracy: 0.3029 - val_loss: 2.2404 - val_accuracy: 0.2129\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 41s 650ms/step - loss: 0.3610 - accuracy: 0.3040 - val_loss: 2.2398 - val_accuracy: 0.2138\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 0.3534 - accuracy: 0.3048 - val_loss: 2.2446 - val_accuracy: 0.2116\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 41s 646ms/step - loss: 0.3473 - accuracy: 0.3061 - val_loss: 2.2437 - val_accuracy: 0.2122\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 41s 651ms/step - loss: 0.3457 - accuracy: 0.3044 - val_loss: 2.2481 - val_accuracy: 0.2136\n",
            "Epoch 92/100\n",
            "63/63 [==============================] - 40s 642ms/step - loss: 0.3382 - accuracy: 0.3080 - val_loss: 2.2532 - val_accuracy: 0.2144\n",
            "Epoch 93/100\n",
            "63/63 [==============================] - 40s 643ms/step - loss: 0.3339 - accuracy: 0.3078 - val_loss: 2.2565 - val_accuracy: 0.2134\n",
            "Epoch 94/100\n",
            "63/63 [==============================] - 40s 641ms/step - loss: 0.3300 - accuracy: 0.3090 - val_loss: 2.2574 - val_accuracy: 0.2140\n",
            "Epoch 95/100\n",
            "63/63 [==============================] - 41s 643ms/step - loss: 0.3216 - accuracy: 0.3117 - val_loss: 2.2572 - val_accuracy: 0.2131\n",
            "Epoch 96/100\n",
            "63/63 [==============================] - 40s 636ms/step - loss: 0.3176 - accuracy: 0.3113 - val_loss: 2.2572 - val_accuracy: 0.2131\n",
            "Epoch 97/100\n",
            "63/63 [==============================] - 40s 640ms/step - loss: 0.3158 - accuracy: 0.3112 - val_loss: 2.2645 - val_accuracy: 0.2151\n",
            "Epoch 98/100\n",
            "63/63 [==============================] - 40s 640ms/step - loss: 0.3085 - accuracy: 0.3138 - val_loss: 2.2596 - val_accuracy: 0.2141\n",
            "Epoch 99/100\n",
            "63/63 [==============================] - 40s 634ms/step - loss: 0.3029 - accuracy: 0.3142 - val_loss: 2.2658 - val_accuracy: 0.2146\n",
            "Epoch 100/100\n",
            "63/63 [==============================] - 40s 629ms/step - loss: 0.2991 - accuracy: 0.3148 - val_loss: 2.2651 - val_accuracy: 0.2147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJHvwiYN0ARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_inputs_model, encoder_outputs)\n",
        "# Encoder hidden states\n",
        "encoder_outputs_as_input = Input(shape=(max_input_len, LATENT_DIM * 2, ))\n",
        "\n",
        "decoder_inputs_single = Input(shape=(1, ))\n",
        "decoder_embeddings_input = decoder_embedding_layer(decoder_inputs_single)\n",
        "\n",
        "# No need to loop through as this is done for 1 time step\n",
        "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
        "\n",
        "# Combine context with last word\n",
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_embeddings_input])\n",
        "\n",
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)\n",
        "\n",
        "decoder_model = Model(inputs=[decoder_inputs_single, encoder_outputs_as_input, initial_s, initial_c], \n",
        "                      outputs=[decoder_outputs, s, c])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czta6gLw0ERk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    enc_hidden_states = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_targets[\"<sos>\"]\n",
        "    eos = word2idx_targets[\"<eos>\"]\n",
        "\n",
        "    s = np.zeros((1, LATENT_DIM))\n",
        "    c = np.zeros((1, LATENT_DIM))\n",
        "\n",
        "    output_sentence = []\n",
        "    for _ in range(max_target_len):\n",
        "        o, s, c = decoder_model.predict([target_seq[0], enc_hidden_states, s, c])\n",
        "        idx = np.argmax(o.flatten())\n",
        "        if idx == eos:\n",
        "            break\n",
        "        word = \"\"\n",
        "        if idx > 0:\n",
        "            word = idx2word_targets[idx]\n",
        "            output_sentence.append(word)\n",
        "        target_seq[0, 0] = idx\n",
        "        \n",
        "    return \" \".join(output_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EnMu9i8JQgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "5fb1ac3e-3a50-46cc-ce80-9eb6e8e4e76b"
      },
      "source": [
        "input_sentence_original = [\"I am fine.\", \"I am happy.\", \"Sure.\"]\n",
        "input_sentence = [re.findall(r\"[A-Za-z]+\", sent) for sent in input_sentence_original]\n",
        "\n",
        "for i, sent in enumerate(input_sentence):\n",
        "    encoded_sentence = []\n",
        "    count = 0\n",
        "    for word in sent:\n",
        "        if count > max_input_len:\n",
        "            break\n",
        "        if word in word2idx_inputs:\n",
        "            idx = word2idx_inputs[word]\n",
        "            encoded_sentence.append(idx)\n",
        "            count += 1\n",
        "\n",
        "    if count < max_input_len:\n",
        "        difference = max_input_len - count \n",
        "        for _ in range(difference):\n",
        "            encoded_sentence.insert(0, 0)\n",
        "    print(\"Original sentence: \", input_sentence_original[i])\n",
        "    print(\"Predicted translation: \", decode_sequence(np.array(encoded_sentence).reshape((1, max_input_len))))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:  I am fine.\n",
            "Predicted translation:  estoy bien.\n",
            "Original sentence:  I am happy.\n",
            "Predicted translation:  estoy feliz.\n",
            "Original sentence:  Sure.\n",
            "Predicted translation:  ¡órale!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}